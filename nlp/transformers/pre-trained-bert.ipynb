{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## [Hugging Face: Basic Task Tutorial for Solving Text Classification Issues](https://www.stxnext.com/blog/hugging-face-tutorial/)\n",
    "\n",
    "To realize the full potential of NLP, you need to be able to use the latest Transformer architecture—a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data.\n",
    "Hugging Face allows you to shorten the distance to the latest NLP solutions and technologies.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BERT\n",
    "\n",
    "It started in 2017 with the introduction of the new Transformer architecture, enabling highly complex and powerful NLP models. Thanks to the Attention mechanism, they didn’t suffer from a lack of training parallelism, as was the case with previous RNN modes.\n",
    "\n",
    "**The BERT model is the encoder part of the Transformer architecture that was trained with the masked language modeling (MLM) and next sentence prediction objective (NSP).**\n",
    "\n",
    "**The GPT model is a part of the decoder in the Transformer architecture.**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading a dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/przemek/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc7a398236db47f08b241e6d45b46b81"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 120000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 7600\n    })\n})"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ag_news = load_dataset(\"ag_news\")\n",
    "ag_news"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'label'],\n    num_rows: 7600\n})"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_news_train = ag_news[\"train\"]\n",
    "ag_news_test = ag_news[\"test\"]\n",
    "ag_news_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\",\n 'label': 2}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_news_test[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Way to see the metadata on the elements of the dataset: the “text” property is of the string type while the label is represented as the ClassLabel. It’s a special class that provides the capabilities to manipulate these class labels and map the label integers to their names with the use of the *int2str* method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label']\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=4, names=['World', 'Sports', 'Business', 'Sci/Tech'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{ag_news_test.column_names}\\n{ag_news_test.features}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if the dataset is balanced:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ag_news[\"train\"].format['type'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Business    30000\n",
      "Sports      30000\n",
      "World       30000\n",
      "Sci/Tech    30000\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ag_news[\"train\"].set_format(\"pandas\")\n",
    "print(f'{ag_news[\"train\"][:][\"label\"].apply(lambda x: ag_news_test.features[\"label\"].int2str(x)).value_counts()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Turns out that it’s perfectly balanced, but this is bound to be a rare situation in the future. In other cases, we will have to take some precautions such as data augmentation, oversampling the minority class, or undersampling the majority class.\n",
    "\n",
    "You should also look at the distribution of sentence lengths across classes. The sentence length will be measured as the number of words in the sentence:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:xlabel='label', ylabel='text_len'>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_ag_news_test = ag_news[\"test\"][:]\n",
    "df_ag_news_train = ag_news[\"train\"][:]\n",
    "df_ag_news_train[\"text_len\"] = df_ag_news_train[\"text\"].apply(lambda x: len(x.split(\" \")))\n",
    "sns.boxplot(data=df_ag_news_train, x=\"label\", y=\"text_len\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The average length of sentences in each class doesn’t exceed 50 words, with extreme values exceeding 200 words in the third class.\n",
    "\n",
    "Our classifier will be created by tuning the BERT language model. It can accept about 512 tokens (**max_position_embeddings (int, optional, defaults to 512) — The maximum sequence length that this model might ever be used with**), and assuming that each word can be decomposed into about 1.5 tokens, in each case the number of tokens in the sentences will not exceed the maximum number accepted by the model. If there were any outliers that had more than 512 tokens, they could be pruned.\n",
    "\n",
    "The only real problem that could arise would be if there were many instances where the length after tokenization would exceed the required 512 tokens. In that case, we would have to treat them differently, but that is beyond the scope of this article."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text tokenization\n",
    "\n",
    "**One of the most important parts of the NLP pipeline is the tokenization of text, as the pre-trained models we’ll use were trained on numerical representations.** For this reason, in order to perform the fine-tuning and inference procedure, we need to convert the text to the same representation on which the models were pretrained.\n",
    "\n",
    "Most modern tokenizers use the subword tokenization technique, which involves splitting words into subword elements, then converting them into identifiers using predefined dictionaries. This ensures that the dictionary isn’t too large, as rare words are modeled with smaller and more common subwords. This approach is good for spelling errors and can be learned from a corpus of data, so common words will be treated as a single token and rare words will be split, making the distribution of tokens in the dataset balanced.\n",
    "\n",
    "The Hugging Face library implements several different subword tokenizers that come with different models:\n",
    "\n",
    "* the WordPiece used by the BERT encoder,\n",
    "* the BPE (Byte-Pair Encoding) used by the RoBERT model,\n",
    "* the SentencePiece used by the XLM encoder.\n",
    "\n",
    "**You need to make sure that the tokenizer you use is compatible with the model it was trained on.**\n",
    "\n",
    "We’ll use the AutoTokenizer class to tokenize the text, which requires a model ID from the Hugging Face Hub or a local path to the model to automatically load the appropriate tokenizer. Since we’ll be using the cased version of the BERT-base model, the code that loads the tokenizer is as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = tokenizer(ag_news[\"train\"]['text'], padding=True, truncation=True)\n",
    "test_tokenized = tokenizer(ag_news[\"test\"]['text'], padding=True, truncation=True)\n",
    "print(\"Tokenized\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The padding parameter determines whether examples will be padded to longer examples from the current batch, while the truncation parameter allows the tokenizer to truncate examples longer than the maximum length, which, in this case, is 512 tokens."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the classifier\n",
    "\n",
    "The most effective way to train a classifier in NLP is to perform supervised fine-tuning of a pretrained encoder for the downstream task using domain-specific data.\n",
    "The tokenized text is fed to a pre-trained encoder that returns an array of token embeddings. We get a separate embedding for each token.\n",
    "\n",
    "So, how do we train the classifier? We can do this in two ways. The first is to **treat the model encoder as a feature extractor**. The second is to **fine-tune the entire model using an attached classifier head**.\n",
    "\n",
    "#### Feature extractor\n",
    "\n",
    "In this case, we extract token embeddings from the given text with the use of the pretrained model. We treat the extracted token embeddings as the text representations and train a classifier directly on them.\n",
    "\n",
    "We don’t change the weights of the encoder. This is a great approach when we don’t have a GPU available, as it’s much less computationally intensive.\n",
    "\n",
    "#### Fine tuning\n",
    "\n",
    "Previously, we treated our BERT language model as a feature extractor that created a sentence representation for later use in another machine learning model.\n",
    "\n",
    "Now, we will attach a dense layer to the BERT language model as a classifier, and during training we’ll update the weight of not only the added layer, but the entire model.\n",
    "\n",
    "We should create a dataset that can be accepted by the Hugging Face trainer class."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=512)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(ag_news[\"train\"][\"text\"], ag_news[\"train\"][\"label\"])\n",
    "test_dataset = Dataset(ag_news[\"test\"][\"text\"], ag_news[\"test\"][\"label\"])\n",
    "print(\"Datasets created.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = (AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4).to(\"cuda\"))\n",
    "print(\"Model.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Helper function that returns the current performance of our model at the time of evaluation in terms of predefined scores:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import numpy as np\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, pos_label='positive', average='weighted')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, pos_label='positive', average='weighted')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, pos_label='positive', average='weighted')\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we need to define the basic parameters of the deep learning process and import a special Trainer class that performs the training, since it has previously implemented all the training loops."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "batch_size = 32\n",
    "logging_steps = len(ag_news[\"train\"]) // batch_size\n",
    "model_names = f\"{'distilbert-base-uncased'}-fine-tuned-ag-news\"\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    seed=0,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We’re left with initializing a Trainer class with a predefined dataset, model, metrics, and training arguments."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/przemek/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/transformers/integrations.py:769: FutureWarning: MLflow support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use MLflow will continue to work without modification, but Python 3.6 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3.7 or newer.\n",
      "  import mlflow\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/przemek/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 120000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7500\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasGemmEx( handle, opa, opb, m, n, k, &falpha, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, &fbeta, c, CUDA_R_16F, ldc, CUDA_R_32F, CUBLAS_GEMM_DFALT_TENSOR_OP)`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-bb7f547e1a7b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mevaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1420\u001B[0m                         \u001B[0mtr_loss_step\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1421\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1422\u001B[0;31m                     \u001B[0mtr_loss_step\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1423\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1424\u001B[0m                 if (\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mtraining_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   2009\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2010\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautocast_smart_context_manager\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2011\u001B[0;31m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompute_loss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2012\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2013\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_gpu\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mcompute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   2041\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2042\u001B[0m             \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2043\u001B[0;31m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2044\u001B[0m         \u001B[0;31m# Save past state if it exists\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2045\u001B[0m         \u001B[0;31m# TODO: this needs to be fixed and made cleaner later.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    756\u001B[0m             \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    757\u001B[0m             \u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 758\u001B[0;31m             \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreturn_dict\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    759\u001B[0m         )\n\u001B[1;32m    760\u001B[0m         \u001B[0mhidden_state\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdistilbert_output\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m  \u001B[0;31m# (bs, seq_len, dim)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    575\u001B[0m             \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    576\u001B[0m             \u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 577\u001B[0;31m             \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreturn_dict\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    578\u001B[0m         )\n\u001B[1;32m    579\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    348\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    349\u001B[0m             layer_outputs = layer_module(\n\u001B[0;32m--> 350\u001B[0;31m                 \u001B[0mx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhidden_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattn_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattn_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhead_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhead_mask\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    351\u001B[0m             )\n\u001B[1;32m    352\u001B[0m             \u001B[0mhidden_state\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlayer_outputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001B[0m\n\u001B[1;32m    301\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    302\u001B[0m         \u001B[0;31m# Feed Forward Network\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 303\u001B[0;31m         \u001B[0mffn_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mffn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msa_output\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# (bs, seq_length, dim)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    304\u001B[0m         \u001B[0mffn_output\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moutput_layer_norm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mffn_output\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0msa_output\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# (bs, seq_length, dim)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    305\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    246\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    247\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 248\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mapply_chunking_to_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mff_chunk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mchunk_size_feed_forward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mseq_len_dim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    249\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    250\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mff_chunk\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/transformers/modeling_utils.py\u001B[0m in \u001B[0;36mapply_chunking_to_forward\u001B[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[1;32m   2926\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_chunks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mchunk_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2927\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2928\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mforward_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput_tensors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001B[0m in \u001B[0;36mff_chunk\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    250\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mff_chunk\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 251\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlin1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    252\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mactivation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    253\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlin2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    102\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 103\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    104\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    105\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Deep Learning/road-to-deep-learning/road-to-deep-learning/venv/lib/python3.6/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mlinear\u001B[0;34m(input, weight, bias)\u001B[0m\n\u001B[1;32m   1846\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mhas_torch_function_variadic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1847\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mhandle_torch_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlinear\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1848\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1849\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1850\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasGemmEx( handle, opa, opb, m, n, k, &falpha, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, &fbeta, c, CUDA_R_16F, ldc, CUDA_R_32F, CUBLAS_GEMM_DFALT_TENSOR_OP)`"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "res = trainer.evaluate()\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}