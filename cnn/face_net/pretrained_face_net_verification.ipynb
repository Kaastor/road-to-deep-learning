{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Load FaceNet Model\n",
    "\n",
    "[Model](https://github.com/nyoki-mtl/keras-facenet)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "[<KerasTensor: shape=(None, 160, 160, 3) dtype=float32 (created by layer 'input_1')>]\n",
      "[<KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'Bottleneck_BatchNorm')>]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "model_file = file_io.FileIO('gs://shield-phishing/face_verification/pretrained_models/face_net_nyoki/facenet.h5', mode='rb')\n",
    "with open(\"./temp_face_model.h5\",'wb') as f:\n",
    "    f.write(model_file.read())\n",
    "    model_file.close()\n",
    "    model = tf.keras.models.load_model(f.name)\n",
    "    os.remove(f.name)\n",
    "\n",
    "print(model.inputs)\n",
    "print(model.outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model('./inception_model/hiroki/facenet_keras.h5')\n",
    "\n",
    "print(model.inputs)\n",
    "print(model.outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def img_to_encoding(image_path, model):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(160, 160))\n",
    "    img = np.around(np.array(img) / 255.0, decimals=12)\n",
    "    x_train = np.expand_dims(img, axis=0)\n",
    "    embedding = model.predict_on_batch(x_train)\n",
    "\n",
    "    return embedding / np.linalg.norm(embedding, ord=2)  # rescale the encoding vectors to have L2 norm equal to one"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def verify(encoding_1, encoding_2):\n",
    "    # Compute distance\n",
    "    dist = np.linalg.norm(encoding_1 - encoding_2)\n",
    "\n",
    "    if dist < 0.7:\n",
    "        verified = True\n",
    "    else:\n",
    "        verified = False\n",
    "    print(\"dist\", dist)\n",
    "\n",
    "    return verified"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5436483, True)\n",
      "(0.386163, True)\n",
      "(1.3963861, False)\n"
     ]
    }
   ],
   "source": [
    "print(verify(img_to_encoding(\"./tutorial/images/camera_1.jpg\", model), img_to_encoding(\"./tutorial/images/bertrand.jpg\", model)))\n",
    "print(verify(img_to_encoding(\"./tutorial/images/camera_3.jpg\", model), img_to_encoding(\"./tutorial/images/bertrand.jpg\", model)))\n",
    "print(verify(img_to_encoding(\"./tutorial/images/camera_1.jpg\", model), img_to_encoding(\"./tutorial/images/younes.jpg\", model)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ways to improve your facial recognition model\n",
    "\n",
    "- Put more images of each person (under different lighting conditions, taken on different days, etc.) into the database. Then, given a new image, compare the new face to multiple pictures of the person. This would increase accuracy.\n",
    "- Crop the images to contain just the face, and less of the \"border\" region around the face. This preprocessing removes some of the irrelevant pixels around the face, and also makes the algorithm more robust."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}